server:
  port: 9090

spring:
  application:
    name: chatclient

  #  ai:
  #    openai:
  #        api-key: ${OPENAI_API_KEY}
  #        chat:
  #          project-id: proj_FHS0jO5kT02JiTvEbsXvADaF
  #          organization_id: org-sHpKa8rszcNPvXmdbTSMK6z7
  #          options:
  #              model: gpt-4o-mini
  ai:
    ollama:
      chat:
        options:
          #              model: llama3.2
          model: qwen3:4b
          temperature: 0.2
    mcp:
      client:
        name: chatclient
        initalized: true
        request-timeout: 20s
        type: sync
        toolcallback.enabled: true
        root-change-notification: true
        sse:
          connections:
            server1:
              url: http://localhost:8080
              sse-endpoint: /api/v1/sse

logging:
  level:
    io.modelcontextprotocol: TRACE
    io.springframework.ai.mcp: TRACE